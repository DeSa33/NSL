# NSL Example 02: Neural Network Basics
# Implementing neural network primitives from scratch

# ============================================
# Activation Functions
# ============================================

fn relu(x) {
    if (x > 0) {
        return x
    }
    return 0.0
}

fn sigmoid(x) {
    # Approximate sigmoid for numerical stability
    if (x > 5) { return 1.0 }
    if (x < -5) { return 0.0 }
    return 1.0 / (1.0 + exp(-x))
}

fn tanh_approx(x) {
    # Approximation: tanh(x) â‰ˆ x(1 - xÂ²/3) for small x
    if (x > 2) { return 1.0 }
    if (x < -2) { return -1.0 }
    let x2 = x * x
    return x * (1.0 - x2 / 3.0)
}

# ============================================
# Basic Perceptron
# ============================================

fn perceptron(inputs, weights, bias) {
    let sum = bias
    for i in range(0, len(inputs)) {
        sum = sum + inputs[i] * weights[i]
    }
    return relu(sum)
}

# Test: AND gate
print("=== AND Gate ===")
let and_weights = [1.0, 1.0]
let and_bias = -1.5

print("AND(0,0) =", perceptron([0, 0], and_weights, and_bias))
print("AND(0,1) =", perceptron([0, 1], and_weights, and_bias))
print("AND(1,0) =", perceptron([1, 0], and_weights, and_bias))
print("AND(1,1) =", perceptron([1, 1], and_weights, and_bias))

# Test: OR gate
print("\n=== OR Gate ===")
let or_weights = [1.0, 1.0]
let or_bias = -0.5

print("OR(0,0) =", perceptron([0, 0], or_weights, or_bias))
print("OR(0,1) =", perceptron([0, 1], or_weights, or_bias))
print("OR(1,0) =", perceptron([1, 0], or_weights, or_bias))
print("OR(1,1) =", perceptron([1, 1], or_weights, or_bias))

# ============================================
# Simple Neural Network Layer
# ============================================

fn dense_layer(inputs, weights_matrix, biases, activation) {
    let outputs = []
    let num_neurons = len(biases)

    for i in range(0, num_neurons) {
        let sum = biases[i]
        for j in range(0, len(inputs)) {
            sum = sum + inputs[j] * weights_matrix[i][j]
        }

        # Apply activation
        if (activation == "relu") {
            outputs = outputs + [relu(sum)]
        } else if (activation == "sigmoid") {
            outputs = outputs + [sigmoid(sum)]
        } else {
            outputs = outputs + [sum]
        }
    }

    return outputs
}

# Test dense layer
print("\n=== Dense Layer Test ===")
let layer_inputs = [1.0, 2.0]
let layer_weights = [
    [0.5, -0.5],
    [0.3, 0.7]
]
let layer_biases = [0.1, -0.1]

let layer_output = dense_layer(layer_inputs, layer_weights, layer_biases, "relu")
print("Dense layer output:", layer_output)

# ============================================
# Gradient Descent Step
# ============================================

fn gradient_step(params, grads, learning_rate) {
    let updated = []
    for i in range(0, len(params)) {
        updated = updated + [params[i] - learning_rate * grads[i]]
    }
    return updated
}

print("\n=== Gradient Descent ===")
let weights = [0.5, 0.3]
let gradients = [0.1, -0.05]
let lr = 0.01

let new_weights = gradient_step(weights, gradients, lr)
print("Original weights:", weights)
print("Gradients:", gradients)
print("Updated weights:", new_weights)

# ============================================
# Loss Functions
# ============================================

fn mse_loss(predictions, targets) {
    let sum = 0.0
    let n = len(predictions)
    for i in range(0, n) {
        let diff = predictions[i] - targets[i]
        sum = sum + diff * diff
    }
    return sum / n
}

print("\n=== MSE Loss ===")
let preds = [0.5, 0.8, 0.3]
let targets = [0.6, 0.9, 0.2]
print("Predictions:", preds)
print("Targets:", targets)
print("MSE Loss:", mse_loss(preds, targets))
