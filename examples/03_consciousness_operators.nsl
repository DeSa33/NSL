# NSL Example 03: Consciousness Operators
# Demonstrating the unique neural-symbolic operators

# ============================================
# Holographic Operator (◈) - Attention
# ============================================
# The holographic operator creates distributed representations
# using attention mechanisms. Think of it as "focusing" on
# the most relevant parts of the input.

print("=== Holographic Operator (◈) ===")

# Basic holographic encoding
let data = 42.5
let encoded = ◈[data]
print("Original:", data)
print("Holographic encoding:", encoded)

# You can also use ASCII alias: holo, holographic, attend
# let encoded_ascii = holo(data)  # Same as ◈[data]

# Multi-dimensional attention
let sequence = [1.0, 2.0, 3.0, 4.0, 5.0]
let attended = ◈[sequence]
print("Sequence:", sequence)
print("Attended representation:", attended)

# ============================================
# Gradient Operator (∇) - Differentiation
# ============================================
# The gradient operator computes derivatives automatically.
# Essential for training neural networks via backpropagation.

print("\n=== Gradient Operator (∇) ===")

# Compute gradient of a value
let loss = 0.5
let grad_loss = ∇[loss]
print("Loss:", loss)
print("Gradient:", grad_loss)

# You can also use ASCII alias: grad, gradient, nabla
# let grad_ascii = grad(loss)  # Same as ∇[loss]

# Gradient of a function (conceptual)
# f(x) = x² → f'(x) = 2x
fn square(x) {
    return x * x
}

let x = 3.0
let fx = square(x)
print("f(x) = x² at x=3:", fx)
print("Gradient ∇f should be 2x = 6")

# ============================================
# Tensor Product (⊗) - Composition
# ============================================
# The tensor product creates combined representations.
# Used for binding concepts, computing outer products,
# and building compositional structures.

print("\n=== Tensor Product Operator (⊗) ===")

# Outer product of two vectors
let a = [1.0, 2.0, 3.0]
let b = [4.0, 5.0]
let outer = ⊗[a, b]
print("Vector a:", a)
print("Vector b:", b)
print("Outer product a ⊗ b:", outer)

# You can also use ASCII alias: outer, kron, tensorprod
# let outer_ascii = outer(a, b)  # Same as ⊗[a, b]

# Role-filler binding (conceptual)
# Bind "agent" role to "John" concept
# let role_agent = random_vector(256)
# let concept_john = embed("John")
# let binding = role_agent ⊗ concept_john

# ============================================
# Quantum Branching (Ψ) - Superposition
# ============================================
# The quantum branching operator creates superposition states.
# Multiple computation paths exist simultaneously and
# can interfere with each other.

print("\n=== Quantum Branching Operator (Ψ) ===")

# Create superposition of states
let state = 1.5
let superposition = Ψ[state]
print("Initial state:", state)
print("Superposition:", superposition)

# You can also use ASCII alias: psi, quantum, superpose
# let superposed_ascii = psi(state)  # Same as Ψ[state]

# Quantum branching for exploration (conceptual)
# Explore multiple hypotheses simultaneously:
# let hypotheses = Ψ {
#     0.5: hypothesis_a,
#     0.3: hypothesis_b,
#     0.2: hypothesis_c
# }

# ============================================
# Combining Operators
# ============================================
# Operators can be freely composed!

print("\n=== Operator Composition ===")

# Gradient of holographic projection
let input = [1.0, 2.0, 3.0]
let holo_input = ◈[input]
let grad_holo = ∇[holo_input]
print("Input:", input)
print("◈(input):", holo_input)
print("∇(◈(input)):", grad_holo)

# Tensor product of quantum states
let s1 = Ψ[1.0]
let s2 = Ψ[2.0]
# Conceptual: entangled = s1 ⊗ s2

# ============================================
# Practical Pattern: Attention Layer
# ============================================

print("\n=== Practical: Self-Attention Pattern ===")

fn self_attention(queries, keys, values) {
    # Compute attention scores
    let scores = []
    for i in range(0, len(queries)) {
        let row = []
        for j in range(0, len(keys)) {
            # Dot product
            let score = 0.0
            for k in range(0, len(queries[i])) {
                score = score + queries[i][k] * keys[j][k]
            }
            row = row + [score]
        }
        scores = scores + [row]
    }

    # Softmax and weighted sum (simplified)
    # In practice: ◈ does this automatically
    print("Attention scores computed")
    return scores
}

# Example usage
let q = [[1.0, 0.0], [0.0, 1.0]]
let k = [[1.0, 0.0], [0.0, 1.0]]
let v = [[1.0, 2.0], [3.0, 4.0]]

let attention = self_attention(q, k, v)
print("Self-attention result:", attention)

print("\n=== Consciousness operators enable neural-symbolic fusion! ===")
