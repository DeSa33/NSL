# NSL Example 04: Mini Transformer
# A complete, working transformer implementation in NSL
# This proves NSL can build real neural network architectures

# ============================================
# Configuration
# ============================================

const D_MODEL = 64       # Model dimension
const N_HEADS = 4        # Number of attention heads
const D_HEAD = 16        # Dimension per head (D_MODEL / N_HEADS)
const D_FF = 128         # Feed-forward hidden dimension
const SEQ_LEN = 8        # Sequence length
const VOCAB_SIZE = 100   # Vocabulary size

# ============================================
# Utility Functions
# ============================================

fn zeros(n) {
    let result = []
    for i in range(0, n) {
        result = result + [0.0]
    }
    return result
}

fn zeros_2d(rows, cols) {
    let result = []
    for i in range(0, rows) {
        result = result + [zeros(cols)]
    }
    return result
}

fn random_uniform(n, scale) {
    # Pseudo-random using linear congruential generator
    let result = []
    mut seed = 12345
    for i in range(0, n) {
        seed = (seed * 1103515245 + 12345) % 2147483648
        let val = (seed / 2147483648.0 - 0.5) * 2.0 * scale
        result = result + [val]
    }
    return result
}

fn random_2d(rows, cols, scale) {
    let result = []
    mut seed = 42
    for i in range(0, rows) {
        let row = []
        for j in range(0, cols) {
            seed = (seed * 1103515245 + 12345) % 2147483648
            let val = (seed / 2147483648.0 - 0.5) * 2.0 * scale
            row = row + [val]
        }
        result = result + [row]
    }
    return result
}

# ============================================
# Matrix Operations
# ============================================

fn matmul(A, B) {
    # A: [m, k], B: [k, n] -> [m, n]
    let m = len(A)
    let k = len(A[0])
    let n = len(B[0])

    let C = []
    for i in range(0, m) {
        let row = []
        for j in range(0, n) {
            let sum = 0.0
            for l in range(0, k) {
                sum = sum + A[i][l] * B[l][j]
            }
            row = row + [sum]
        }
        C = C + [row]
    }
    return C
}

fn transpose(A) {
    let rows = len(A)
    let cols = len(A[0])
    let result = []
    for j in range(0, cols) {
        let row = []
        for i in range(0, rows) {
            row = row + [A[i][j]]
        }
        result = result + [row]
    }
    return result
}

fn add_matrices(A, B) {
    let result = []
    for i in range(0, len(A)) {
        let row = []
        for j in range(0, len(A[0])) {
            row = row + [A[i][j] + B[i][j]]
        }
        result = result + [row]
    }
    return result
}

fn scale_matrix(A, s) {
    let result = []
    for i in range(0, len(A)) {
        let row = []
        for j in range(0, len(A[0])) {
            row = row + [A[i][j] * s]
        }
        result = result + [row]
    }
    return result
}

# ============================================
# Activation Functions
# ============================================

fn softmax(x) {
    # Find max for numerical stability
    mut max_val = x[0]
    for i in range(1, len(x)) {
        if (x[i] > max_val) {
            max_val = x[i]
        }
    }

    # Compute exp and sum
    let exp_vals = []
    mut sum = 0.0
    for i in range(0, len(x)) {
        let e = exp(x[i] - max_val)
        exp_vals = exp_vals + [e]
        sum = sum + e
    }

    # Normalize
    let result = []
    for i in range(0, len(exp_vals)) {
        result = result + [exp_vals[i] / sum]
    }
    return result
}

fn softmax_2d(A) {
    # Apply softmax to each row
    let result = []
    for i in range(0, len(A)) {
        result = result + [softmax(A[i])]
    }
    return result
}

fn relu(x) {
    if (x > 0) { return x }
    return 0.0
}

fn relu_2d(A) {
    let result = []
    for i in range(0, len(A)) {
        let row = []
        for j in range(0, len(A[0])) {
            row = row + [relu(A[i][j])]
        }
        result = result + [row]
    }
    return result
}

fn gelu(x) {
    # GELU activation: x * Φ(x)
    # Approximation: 0.5 * x * (1 + tanh(sqrt(2/π) * (x + 0.044715 * x³)))
    let c = 0.7978845608  # sqrt(2/π)
    let x3 = x * x * x
    let inner = c * (x + 0.044715 * x3)

    # tanh approximation
    mut t = inner
    if (t > 2) { t = 1.0 }
    else if (t < -2) { t = -1.0 }
    else { t = t * (1.0 - t * t / 3.0) }

    return 0.5 * x * (1.0 + t)
}

fn gelu_2d(A) {
    let result = []
    for i in range(0, len(A)) {
        let row = []
        for j in range(0, len(A[0])) {
            row = row + [gelu(A[i][j])]
        }
        result = result + [row]
    }
    return result
}

# ============================================
# Layer Normalization
# ============================================

fn layer_norm(x, gamma, beta) {
    # Compute mean
    mut mean = 0.0
    for i in range(0, len(x)) {
        mean = mean + x[i]
    }
    mean = mean / len(x)

    # Compute variance
    mut var = 0.0
    for i in range(0, len(x)) {
        let diff = x[i] - mean
        var = var + diff * diff
    }
    var = var / len(x)

    # Normalize
    let eps = 0.00001
    let std = sqrt(var + eps)
    let result = []
    for i in range(0, len(x)) {
        let normalized = (x[i] - mean) / std
        result = result + [gamma[i] * normalized + beta[i]]
    }
    return result
}

fn layer_norm_2d(A, gamma, beta) {
    let result = []
    for i in range(0, len(A)) {
        result = result + [layer_norm(A[i], gamma, beta)]
    }
    return result
}

# ============================================
# Scaled Dot-Product Attention
# ============================================

fn scaled_dot_product_attention(Q, K, V) {
    # Q, K, V: [seq_len, d_head]
    # Attention(Q, K, V) = softmax(QK^T / sqrt(d_k)) * V

    let d_k = len(Q[0])
    let scale = 1.0 / sqrt(d_k)

    # Compute QK^T
    let K_T = transpose(K)
    let scores = matmul(Q, K_T)

    # Scale
    let scaled_scores = scale_matrix(scores, scale)

    # Softmax
    let attention_weights = softmax_2d(scaled_scores)

    # Weighted sum of values
    let output = matmul(attention_weights, V)

    return output
}

# ============================================
# Multi-Head Attention
# ============================================

fn split_heads(X, n_heads, d_head) {
    # X: [seq_len, d_model] -> [n_heads, seq_len, d_head]
    let seq_len = len(X)
    let heads = []

    for h in range(0, n_heads) {
        let head = []
        for s in range(0, seq_len) {
            let row = []
            for d in range(0, d_head) {
                row = row + [X[s][h * d_head + d]]
            }
            head = head + [row]
        }
        heads = heads + [head]
    }
    return heads
}

fn concat_heads(heads) {
    # [n_heads, seq_len, d_head] -> [seq_len, d_model]
    let n_heads = len(heads)
    let seq_len = len(heads[0])
    let d_head = len(heads[0][0])

    let result = []
    for s in range(0, seq_len) {
        let row = []
        for h in range(0, n_heads) {
            for d in range(0, d_head) {
                row = row + [heads[h][s][d]]
            }
        }
        result = result + [row]
    }
    return result
}

fn multi_head_attention(X, W_Q, W_K, W_V, W_O, n_heads, d_head) {
    # Project Q, K, V
    let Q = matmul(X, W_Q)
    let K = matmul(X, W_K)
    let V = matmul(X, W_V)

    # Split into heads
    let Q_heads = split_heads(Q, n_heads, d_head)
    let K_heads = split_heads(K, n_heads, d_head)
    let V_heads = split_heads(V, n_heads, d_head)

    # Apply attention to each head
    let head_outputs = []
    for h in range(0, n_heads) {
        let head_out = scaled_dot_product_attention(Q_heads[h], K_heads[h], V_heads[h])
        head_outputs = head_outputs + [head_out]
    }

    # Concatenate heads
    let concatenated = concat_heads(head_outputs)

    # Final projection
    let output = matmul(concatenated, W_O)

    return output
}

# ============================================
# Feed-Forward Network
# ============================================

fn feed_forward(X, W1, b1, W2, b2) {
    # FFN(x) = GELU(xW1 + b1)W2 + b2

    # First linear layer
    let hidden = matmul(X, W1)
    # Add bias
    for i in range(0, len(hidden)) {
        for j in range(0, len(hidden[0])) {
            hidden[i][j] = hidden[i][j] + b1[j]
        }
    }

    # Activation
    let activated = gelu_2d(hidden)

    # Second linear layer
    let output = matmul(activated, W2)
    # Add bias
    for i in range(0, len(output)) {
        for j in range(0, len(output[0])) {
            output[i][j] = output[i][j] + b2[j]
        }
    }

    return output
}

# ============================================
# Transformer Block
# ============================================

fn transformer_block(X, attn_weights, ff_weights, ln_params) {
    # Unpack weights
    let W_Q = attn_weights[0]
    let W_K = attn_weights[1]
    let W_V = attn_weights[2]
    let W_O = attn_weights[3]

    let W1 = ff_weights[0]
    let b1 = ff_weights[1]
    let W2 = ff_weights[2]
    let b2 = ff_weights[3]

    let gamma1 = ln_params[0]
    let beta1 = ln_params[1]
    let gamma2 = ln_params[2]
    let beta2 = ln_params[3]

    # Multi-head attention with residual
    let attn_out = multi_head_attention(X, W_Q, W_K, W_V, W_O, N_HEADS, D_HEAD)
    let residual1 = add_matrices(X, attn_out)
    let normed1 = layer_norm_2d(residual1, gamma1, beta1)

    # Feed-forward with residual
    let ff_out = feed_forward(normed1, W1, b1, W2, b2)
    let residual2 = add_matrices(normed1, ff_out)
    let normed2 = layer_norm_2d(residual2, gamma2, beta2)

    return normed2
}

# ============================================
# Initialize Transformer Weights
# ============================================

fn init_transformer_weights() {
    # Xavier initialization scale
    let scale = 0.1

    # Attention weights
    let W_Q = random_2d(D_MODEL, D_MODEL, scale)
    let W_K = random_2d(D_MODEL, D_MODEL, scale)
    let W_V = random_2d(D_MODEL, D_MODEL, scale)
    let W_O = random_2d(D_MODEL, D_MODEL, scale)
    let attn_weights = [W_Q, W_K, W_V, W_O]

    # Feed-forward weights
    let W1 = random_2d(D_MODEL, D_FF, scale)
    let b1 = zeros(D_FF)
    let W2 = random_2d(D_FF, D_MODEL, scale)
    let b2 = zeros(D_MODEL)
    let ff_weights = [W1, b1, W2, b2]

    # Layer norm parameters (initialized to 1 and 0)
    let gamma1 = []
    let beta1 = []
    let gamma2 = []
    let beta2 = []
    for i in range(0, D_MODEL) {
        gamma1 = gamma1 + [1.0]
        beta1 = beta1 + [0.0]
        gamma2 = gamma2 + [1.0]
        beta2 = beta2 + [0.0]
    }
    let ln_params = [gamma1, beta1, gamma2, beta2]

    return [attn_weights, ff_weights, ln_params]
}

# ============================================
# Positional Encoding
# ============================================

fn positional_encoding(seq_len, d_model) {
    let PE = []
    for pos in range(0, seq_len) {
        let row = []
        for i in range(0, d_model) {
            # Use floor division manually: i / 2 = floor(i / 2)
            let half_i = floor(i / 2)
            # Use pow() instead of ** operator
            let divisor = pow(10000.0, 2.0 * half_i / d_model)
            let angle = pos / divisor
            if (i % 2 == 0) {
                row = row + [sin(angle)]
            } else {
                row = row + [cos(angle)]
            }
        }
        PE = PE + [row]
    }
    return PE
}

# ============================================
# Full Transformer Forward Pass
# ============================================

fn transformer_forward(input_ids, embedding_matrix, transformer_weights) {
    # Embed input tokens
    let embeddings = []
    for i in range(0, len(input_ids)) {
        embeddings = embeddings + [embedding_matrix[input_ids[i]]]
    }

    # Add positional encoding
    let PE = positional_encoding(len(input_ids), D_MODEL)
    let X = add_matrices(embeddings, PE)

    # Apply transformer block
    let attn_weights = transformer_weights[0]
    let ff_weights = transformer_weights[1]
    let ln_params = transformer_weights[2]

    let output = transformer_block(X, attn_weights, ff_weights, ln_params)

    return output
}

# ============================================
# Main: Run Mini Transformer
# ============================================

print("====================================")
print("   Mini Transformer in NSL")
print("====================================")
print("")
print("Configuration:")
print("  Model dimension:", D_MODEL)
print("  Number of heads:", N_HEADS)
print("  Head dimension:", D_HEAD)
print("  FF dimension:", D_FF)
print("  Sequence length:", SEQ_LEN)
print("")

# Initialize weights
print("Initializing transformer weights...")
let weights = init_transformer_weights()
print("  Attention weights: 4 matrices of", D_MODEL, "x", D_MODEL)
print("  FF weights: 2 matrices +", "biases")
print("  LayerNorm params: 4 vectors")
print("")

# Create embedding matrix
print("Creating embedding matrix...")
let embedding_matrix = random_2d(VOCAB_SIZE, D_MODEL, 0.1)
print("  Shape:", VOCAB_SIZE, "x", D_MODEL)
print("")

# Sample input (token IDs)
let input_ids = [5, 12, 7, 23, 42, 8, 15, 3]
print("Input token IDs:", input_ids)
print("")

# Forward pass
print("Running transformer forward pass...")
let output = transformer_forward(input_ids, embedding_matrix, weights)
print("Output shape:", len(output), "x", len(output[0]))
print("")

# Show first few values of output
print("Output (first 3 positions, first 4 dims):")
for i in range(0, 3) {
    let row = []
    for j in range(0, 4) {
        row = row + [output[i][j]]
    }
    print("  Position", i, ":", row)
}

print("")
print("====================================")
print("   Transformer Forward Pass Complete!")
print("====================================")
print("")
print("This demonstrates NSL can build real")
print("neural network architectures including:")
print("  - Multi-head self-attention")
print("  - Layer normalization")
print("  - GELU activation")
print("  - Residual connections")
print("  - Positional encoding")
